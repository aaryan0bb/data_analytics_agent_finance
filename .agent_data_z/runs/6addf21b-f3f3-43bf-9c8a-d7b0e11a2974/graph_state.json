{"initial_state": {"user_input": "Is insider buying a leading signal? Rank stocks by net insider buys in the last 90 days and run a 3-month forward performance test.", "task_description": "", "selected_tools": [], "retrieved_few_shots": [], "composite_prompt": "", "generated_code": "", "code_explanation": "", "execution_result": {}, "error_message": "", "reflection_count": 0, "final_response": "", "intermediate_results": {}, "tool_params": {}, "tool_results": {}, "tool_files": {"insider_trades": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet"}, "data_dir": "", "tool_calls_log": [], "tool_outputs": {"insider_trades": {"ok": true, "file_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "rows": 500, "columns": 16, "dtypes": {"symbol": "object", "filingDate": "object", "transactionDate": "object", "reportingCik": "object", "companyCik": "object", "transactionType": "object", "securitiesOwned": "float64", "reportingName": "object", "typeOfOwner": "object", "acquisitionOrDisposition": "object", "directOrIndirect": "object", "formType": "object", "securitiesTransacted": "float64", "price": "float64", "securityName": "object", "url": "object"}, "nulls": {"symbol": 0, "filingDate": 0, "transactionDate": 0, "reportingCik": 0, "companyCik": 0, "transactionType": 0, "securitiesOwned": 0, "reportingName": 0, "typeOfOwner": 0, "acquisitionOrDisposition": 0, "directOrIndirect": 45, "formType": 0, "securitiesTransacted": 0, "price": 0, "securityName": 0, "url": 0}, "date_start": null, "date_end": null, "sample_head": [{"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634407663.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 27107.0, "price": 237.5579, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634381662.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 26001.0, "price": 238.322, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634377397.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 4265.0, "price": 239.4642, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634373912.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 3485.0, "price": 240.5735, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634371050.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 2862.0, "price": 241.4435, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}]}}, "intermediate_context": "", "run_id": "6addf21b-f3f3-43bf-9c8a-d7b0e11a2974", "warnings": ["Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet"], "iteration_manifests": ["ResultManifest(tables=[TableArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/insider_net_90d_rank_20250926.csv', rows=102, columns=['symbol', 'net_shares_90d', 'net_dollar_90d', 'tx_count', 'buy_count', 'sell_count', 'last_trade_date', 'rank', 'rank_pct'], schema=None, description=None)], figures=[FigureArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/top_net_insider_buys_20250926.html', caption=None, format='html')], metrics={'formation_date': '2025-09-26', 'lookback_days': 90, 'universe_size_90d': 102, 'long_bucket_size': 10, 'short_bucket_size': 0, 'backtest': {'status': 'skipped_missing_prices', 'reason': 'No local price artifacts provided to compute 3-month forward returns.'}, 'frictions_bps': {'slippage': 5.0, 'commission': 1.0}, 'leverage_cap': 1.0, 'allow_shorts': False}, explanation='This analysis ranks stocks by net insider buying (shares * price with A=+1, D=-1) over the last 90 days ending on 2025-09-26. It saves a ranking table and a Plotly bar chart of the top names. A 3-month forward performance backtest is prepared with strict no-lookahead (trade next bar), frictions (slippage and commissions), position limits, and exposure caps; however, it is skipped in this run because no local price artifacts were provided. If price data (with adjusted prices) are supplied locally, the script will compute forward returns, Sharpe, Probabilistic Sharpe Ratio, 95% bootstrap CI for Sharpe, turnover, gross/net exposure, max drawdown, and HHI of weights, and will save the equity and returns time series.')"], "iteration_index": 0, "eval_report": {}, "tavily_results": {}, "actions_synthesis": [], "actions_synthesis_all": [], "delta_plan": {}, "eval_bundle": {}, "delta_tool_outputs": {}, "improvement_round": 0, "should_finalize_after_eval": false, "acceptance_result": {}, "acceptance_failed": false, "last_good_code": "", "last_good_manifest_summary": {}, "last_good_iteration_index": 0, "rollback_info": {}, "unused_tool_picks": [], "unused_tool_files": {}, "unused_tool_outputs": {}, "unused_tool_names": [], "base_data_dir": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z"}, "final_state": {"user_input": "Is insider buying a leading signal? Rank stocks by net insider buys in the last 90 days and run a 3-month forward performance test.", "task_description": "", "selected_tools": ["extract_latest_insider_trades", "bulk_extract_daily_closing_prices_from_polygon", "extract_fundamentals_from_fmp"], "retrieved_few_shots": [{"question": "perform rolling regression (OLS) of factor A on 1 day forward SPY returns with window of 250 days, compute the residuals and calculate the rolling percentile rank of these residuals with a window of 20 days", "executable_code": "import pandas as pd\nimport numpy as np\nfrom statsmodels.regression.rolling import RollingOLS\nimport statsmodels.api as sm\n\n# Tool call example - factor and SPY data would be available in state:\n# state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n# state['factors']['factor_a'] = some_calculated_factor_series\n\ndef perform_rolling_regression_and_percentile_rank(state, factor_key, spy_key='SPY_daily', rolling_window=250, rank_window=20):\n    # Access data from LangGraph state\n    if factor_key not in state.get('factors', {}) and factor_key not in state.get('dataframes', {}):\n        raise ValueError(f'Factor data ({factor_key}) not found in state.')\n    if spy_key not in state.get('dataframes', {}):\n        raise ValueError(f'SPY data ({spy_key}) not found in state.')\n\n    # Get factor and SPY data\n    if factor_key in state.get('factors', {}):\n        factor_data = state['factors'][factor_key]\n    else:\n        factor_df = state['dataframes'][factor_key]\n        factor_data = factor_df.iloc[:, 0] if isinstance(factor_df, pd.DataFrame) else factor_df\n\n    spy_data = state['dataframes'][spy_key]\n    # Calculate 1-day forward returns for SPY\n    forward_returns = spy_data['Close'].pct_change().shift(-1)\n\n    # Merge data on Date index\n    data = pd.concat([factor_data, forward_returns], axis=1)\n    data.columns = ['Factor_A', 'Forward_SPY_Returns']\n    data = data.dropna()\n\n    # Perform rolling OLS\n    model = RollingOLS(endog=data['Forward_SPY_Returns'], exog=sm.add_constant(data['Factor_A']), window=rolling_window)\n    fitted_model = model.fit()\n    data['Residuals'] = fitted_model.resids\n\n    # Calculate rolling percentile rank of residuals\n    data['Residuals_Percentile_Rank'] = data['Residuals'].rolling(window=rank_window).apply(\n        lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=False)\n\n    # Store results back in state\n    state.setdefault('factors', {})[f'{factor_key}_forward_regression_percentile'] = data['Residuals_Percentile_Rank']\n    state.setdefault('analysis_results', {})[f'{factor_key}_forward_regression'] = {\n        'residuals': data['Residuals'],\n        'percentile_ranks': data['Residuals_Percentile_Rank'],\n        'predictive_power': fitted_model.rsquared.mean() if hasattr(fitted_model.rsquared, 'mean') else None\n    }\n\n    return data[['Residuals_Percentile_Rank']]\n\n# Example usage in LangGraph agent:\n# 1. Extract data and calculate factor:\n#    state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n#    state['factors']['momentum_factor'] = some_momentum_calculation\n# 2. Perform forward-looking regression analysis:\npercentile_ranks = perform_rolling_regression_and_percentile_rank(state, 'momentum_factor', rolling_window=250, rank_window=20)\nprint(percentile_ranks)\n", "code_description": "This Python code conducts a rolling regression of Factor A on 1-day forward SPY returns using LangGraph state, enabling predictive analysis of factor performance. It computes residuals and calculates rolling percentile ranks to identify relative performance within rolling windows. Results are stored in state for subsequent agents to use in trading signal generation or risk assessment workflows.", "source_file": "factor_generation.py", "code": "import pandas as pd\nimport numpy as np\nfrom statsmodels.regression.rolling import RollingOLS\nimport statsmodels.api as sm\n\n# Tool call example - factor and SPY data would be available in state:\n# state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n# state['factors']['factor_a'] = some_calculated_factor_series\n\ndef perform_rolling_regression_and_percentile_rank(state, factor_key, spy_key='SPY_daily', rolling_window=250, rank_window=20):\n    # Access data from LangGraph state\n    if factor_key not in state.get('factors', {}) and factor_key not in state.get('dataframes', {}):\n        raise ValueError(f'Factor data ({factor_key}) not found in state.')\n    if spy_key not in state.get('dataframes', {}):\n        raise ValueError(f'SPY data ({spy_key}) not found in state.')\n\n    # Get factor and SPY data\n    if factor_key in state.get('factors', {}):\n        factor_data = state['factors'][factor_key]\n    else:\n        factor_df = state['dataframes'][factor_key]\n        factor_data = factor_df.iloc[:, 0] if isinstance(factor_df, pd.DataFrame) else factor_df\n\n    spy_data = state['dataframes'][spy_key]\n    # Calculate 1-day forward returns for SPY\n    forward_returns = spy_data['Close'].pct_change().shift(-1)\n\n    # Merge data on Date index\n    data = pd.concat([factor_data, forward_returns], axis=1)\n    data.columns = ['Factor_A', 'Forward_SPY_Returns']\n    data = data.dropna()\n\n    # Perform rolling OLS\n    model = RollingOLS(endog=data['Forward_SPY_Returns'], exog=sm.add_constant(data['Factor_A']), window=rolling_window)\n    fitted_model = model.fit()\n    data['Residuals'] = fitted_model.resids\n\n    # Calculate rolling percentile rank of residuals\n    data['Residuals_Percentile_Rank'] = data['Residuals'].rolling(window=rank_window).apply(\n        lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=False)\n\n    # Store results back in state\n    state.setdefault('factors', {})[f'{factor_key}_forward_regression_percentile'] = data['Residuals_Percentile_Rank']\n    state.setdefault('analysis_results', {})[f'{factor_key}_forward_regression'] = {\n        'residuals': data['Residuals'],\n        'percentile_ranks': data['Residuals_Percentile_Rank'],\n        'predictive_power': fitted_model.rsquared.mean() if hasattr(fitted_model.rsquared, 'mean') else None\n    }\n\n    return data[['Residuals_Percentile_Rank']]\n\n# Example usage in LangGraph agent:\n# 1. Extract data and calculate factor:\n#    state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n#    state['factors']['momentum_factor'] = some_momentum_calculation\n# 2. Perform forward-looking regression analysis:\npercentile_ranks = perform_rolling_regression_and_percentile_rank(state, 'momentum_factor', rolling_window=250, rank_window=20)\nprint(percentile_ranks)\n", "description": "This Python code conducts a rolling regression of Factor A on 1-day forward SPY returns using LangGraph state, enabling predictive analysis of factor performance. It computes residuals and calculates rolling percentile ranks to identify relative performance within rolling windows. Results are stored in state for subsequent agents to use in trading signal generation or risk assessment workflows.", "faiss_score": 0.3688632547855377, "rerank_score": -6.991655349731445}, {"question": "Generate momentum factor for the list of stocks, where the momentum factor is calculated as SMA 20 - SMA200", "executable_code": "import pandas as pd\nimport numpy as np\n\n# Tool call example - ticker data would be extracted beforehand:\n# from tools_clean import bulk_extract_daily_closing_prices_from_polygon\n# tickers = ['AAPL', 'MSFT', 'GOOGL']\n# merged_data = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# state['dataframes']['merged_closing_prices'] = merged_data\n\ndef calculate_momentum_factor(state, tickers):\n    momentum_factors = {}\n    \n    for ticker in tickers:\n        # Try to access data from state - check multiple possible keys\n        stock_data = None\n        if 'merged_closing_prices' in state.get('dataframes', {}):\n            merged_df = state['dataframes']['merged_closing_prices']\n            if ticker in merged_df.columns:\n                stock_data = pd.DataFrame({ticker: merged_df[ticker]})\n                stock_data.columns = ['Close']\n        elif f'{ticker}_daily' in state.get('dataframes', {}):\n            stock_data = state['dataframes'][f'{ticker}_daily']\n        \n        if stock_data is None:\n            raise ValueError(f'Stock data for {ticker} not found in state. Ensure data extraction was performed first.')\n        \n        # Calculate SMA-based momentum factor\n        sma_20 = stock_data['Close'].rolling(window=20).mean()\n        sma_200 = stock_data['Close'].rolling(window=200).mean()\n        momentum_factors[ticker] = sma_20 - sma_200\n    \n    momentum_df = pd.DataFrame(momentum_factors)\n    \n    # Store results back in state\n    state.setdefault('factors', {})['sma_momentum'] = momentum_df\n    state.setdefault('signals', {})['sma_momentum_signals'] = momentum_df\n    \n    return momentum_df\n\n# Example usage in LangGraph agent:\n# 1. First extract ticker data:\n#    tickers = ['AAPL', 'MSFT', 'GOOGL']\n#    state['dataframes']['merged_closing_prices'] = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# 2. Then calculate SMA momentum factor:\ntickers = ['AAPL', 'MSFT', 'GOOGL']\nmomentum_factor = calculate_momentum_factor(state, tickers)\nprint(momentum_factor)\n", "code_description": "This Python code computes the SMA-based momentum factor for a list of stocks using LangGraph state. It accesses stock data from the state dict, calculates the difference between 20-day and 200-day simple moving averages for each ticker, and stores results in state under both 'factors' and 'signals'. This momentum signal can be used by subsequent portfolio optimization or trading strategy agents in the LangGraph workflow.", "source_file": "signal_generation.py", "code": "import pandas as pd\nimport numpy as np\n\n# Tool call example - ticker data would be extracted beforehand:\n# from tools_clean import bulk_extract_daily_closing_prices_from_polygon\n# tickers = ['AAPL', 'MSFT', 'GOOGL']\n# merged_data = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# state['dataframes']['merged_closing_prices'] = merged_data\n\ndef calculate_momentum_factor(state, tickers):\n    momentum_factors = {}\n    \n    for ticker in tickers:\n        # Try to access data from state - check multiple possible keys\n        stock_data = None\n        if 'merged_closing_prices' in state.get('dataframes', {}):\n            merged_df = state['dataframes']['merged_closing_prices']\n            if ticker in merged_df.columns:\n                stock_data = pd.DataFrame({ticker: merged_df[ticker]})\n                stock_data.columns = ['Close']\n        elif f'{ticker}_daily' in state.get('dataframes', {}):\n            stock_data = state['dataframes'][f'{ticker}_daily']\n        \n        if stock_data is None:\n            raise ValueError(f'Stock data for {ticker} not found in state. Ensure data extraction was performed first.')\n        \n        # Calculate SMA-based momentum factor\n        sma_20 = stock_data['Close'].rolling(window=20).mean()\n        sma_200 = stock_data['Close'].rolling(window=200).mean()\n        momentum_factors[ticker] = sma_20 - sma_200\n    \n    momentum_df = pd.DataFrame(momentum_factors)\n    \n    # Store results back in state\n    state.setdefault('factors', {})['sma_momentum'] = momentum_df\n    state.setdefault('signals', {})['sma_momentum_signals'] = momentum_df\n    \n    return momentum_df\n\n# Example usage in LangGraph agent:\n# 1. First extract ticker data:\n#    tickers = ['AAPL', 'MSFT', 'GOOGL']\n#    state['dataframes']['merged_closing_prices'] = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# 2. Then calculate SMA momentum factor:\ntickers = ['AAPL', 'MSFT', 'GOOGL']\nmomentum_factor = calculate_momentum_factor(state, tickers)\nprint(momentum_factor)\n", "description": "This Python code computes the SMA-based momentum factor for a list of stocks using LangGraph state. It accesses stock data from the state dict, calculates the difference between 20-day and 200-day simple moving averages for each ticker, and stores results in state under both 'factors' and 'signals'. This momentum signal can be used by subsequent portfolio optimization or trading strategy agents in the LangGraph workflow.", "faiss_score": 0.2728918790817261, "rerank_score": -7.890776634216309}, {"question": "Perform decile analysis for factor A against SPY 1D forward returns; calculate percentile ranks with a rolling window of 60 days for factor A, bin them into 5 equi-spaced percentiles; merge with the SPY returns, then group by (mean) and plot the deciles vs returns as a bar chart using Plotly with dark mode theme.", "executable_code": "import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Load data function\ndef load_data(factor_path, spy_path):\n    factors = pd.read_csv(factor_path, parse_dates=['Date'], index_col='Date')\n    spy_prices = pd.read_csv(spy_path, parse_dates=['Date'], index_col='Date')\n    return factors, spy_prices\n\n# Prepare data function\ndef prepare_data(factors, spy_prices, num_bins):\n    spy_prices['Forward_Return'] = spy_prices['Close'].pct_change().shift(-1)\n    factors['Factor_A_Rank'] = factors['Factor A'].rolling(window=60).apply(lambda x: pd.qcut(x.rank(method='first'), num_bins, labels=range(1, num_bins + 1)))\n    combined_data = factors[['Factor_A_Rank']].join(spy_prices['Forward_Return']).dropna()\n    return combined_data\n\n# Analyze and plot function\ndef analyze_and_plot(data, num_bins):\n    grouped_data = data.groupby('Factor_A_Rank')['Forward_Return'].mean()\n    fig = go.Figure(data=[go.Bar(x=list(range(1, num_bins + 1)), y=grouped_data, marker_color='lightskyblue')])\n    fig.update_layout(\n        title='Decile Analysis of Factor A vs. SPY 1D Forward Returns',\n        xaxis_title='Decile of Factor A',\n        yaxis_title='Average SPY 1D Forward Returns',\n        template='plotly_dark'\n    )\n    fig.show()\n\n# Main execution\nfactor_path = '/path/to/factor.csv'\nspy_path = '/path/to/SPY_daily_ohlcv.csv'\nnum_bins = 5\nfactors, spy_prices = load_data(factor_path, spy_path)\ndata = prepare_data(factors, spy_prices, num_bins)\nanalyze_and_plot(data, num_scores)\n", "code_description": "This Python script uses Plotly to perform a decile (quintile) analysis of Factor A against SPY's 1-day forward returns. It calculates rolling percentile ranks for Factor A over a 60-day window and bins these ranks into 5 equi-spaced percentiles. The script then merges these binned ranks with SPY's forward returns, computes the average return for each bin, and visualizes the results in a bar chart using Plotly's dark mode theme. This visualization helps to understand how different levels of Factor A influence SPY returns, providing a clear and interactive means to assess the factor's predictive power on stock market movements.", "source_file": "simple_visuals.py", "code": "import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Load data function\ndef load_data(factor_path, spy_path):\n    factors = pd.read_csv(factor_path, parse_dates=['Date'], index_col='Date')\n    spy_prices = pd.read_csv(spy_path, parse_dates=['Date'], index_col='Date')\n    return factors, spy_prices\n\n# Prepare data function\ndef prepare_data(factors, spy_prices, num_bins):\n    spy_prices['Forward_Return'] = spy_prices['Close'].pct_change().shift(-1)\n    factors['Factor_A_Rank'] = factors['Factor A'].rolling(window=60).apply(lambda x: pd.qcut(x.rank(method='first'), num_bins, labels=range(1, num_bins + 1)))\n    combined_data = factors[['Factor_A_Rank']].join(spy_prices['Forward_Return']).dropna()\n    return combined_data\n\n# Analyze and plot function\ndef analyze_and_plot(data, num_bins):\n    grouped_data = data.groupby('Factor_A_Rank')['Forward_Return'].mean()\n    fig = go.Figure(data=[go.Bar(x=list(range(1, num_bins + 1)), y=grouped_data, marker_color='lightskyblue')])\n    fig.update_layout(\n        title='Decile Analysis of Factor A vs. SPY 1D Forward Returns',\n        xaxis_title='Decile of Factor A',\n        yaxis_title='Average SPY 1D Forward Returns',\n        template='plotly_dark'\n    )\n    fig.show()\n\n# Main execution\nfactor_path = '/path/to/factor.csv'\nspy_path = '/path/to/SPY_daily_ohlcv.csv'\nnum_bins = 5\nfactors, spy_prices = load_data(factor_path, spy_path)\ndata = prepare_data(factors, spy_prices, num_bins)\nanalyze_and_plot(data, num_scores)\n", "description": "This Python script uses Plotly to perform a decile (quintile) analysis of Factor A against SPY's 1-day forward returns. It calculates rolling percentile ranks for Factor A over a 60-day window and bins these ranks into 5 equi-spaced percentiles. The script then merges these binned ranks with SPY's forward returns, computes the average return for each bin, and visualizes the results in a bar chart using Plotly's dark mode theme. This visualization helps to understand how different levels of Factor A influence SPY returns, providing a clear and interactive means to assess the factor's predictive power on stock market movements.", "faiss_score": 0.3505726158618927, "rerank_score": -8.600173950195312}], "composite_prompt": "\n    You are a quantitative research code generator. You will receive local artifact paths (CSV/Parquet) and must analyze them. Follow the conventions and the output contract strictly.\n\n    \nPlotting conventions:\n- Use Plotly with the 'plotly_dark' template.\n- Prefer log scale for price series; normal scale for returns.\n- For time series, show shaded drawdown regions.\n- Save interactive charts as standalone HTML files under ${DATA_DIR}.\n- Do not call any network APIs in plots (no map tiles, no web requests).\n\n\n    \nBacktesting hygiene (MANDATORY):\n- No look-ahead: when computing signals, only use information available at time t, trade at t+1 open or next bar.\n- Apply realistic frictions: include commissions and slippage; assume a trade delay of one bar.\n- Enforce position limits, leverage caps, and (if shorting) borrow constraints.\n- Corporate actions handled via adjusted prices.\n- Report turnover, gross/net exposures, max drawdown.\n\n\n    \nStatistical rigor:\n- Report Sharpe and Probabilistic Sharpe Ratio (PSR).\n- Bootstrap the equity curve for 2,000 resamples to report 95% CI of Sharpe.\n- Compute hit rate, skew, kurtosis, and HHI of weights if a portfolio.\n- If you try >1 hypothesis/parameter set, state the count and warn about multiple testing (White's Reality Check or SPA suggested).\n\n\n    \nOutput contract (MANDATORY):\n- Save all tabular outputs to CSV/Parquet inside ${DATA_DIR}.\n- Save all plots as Plotly HTML (and PNG/SVG if helpful) inside ${DATA_DIR}.\n- Write a 'result.json' in the working directory with keys: tables, figures, metrics, explanation.\n- Do not access any network resources. Read only the local artifacts provided to you.\n- Each tables[] and figures[] entry MUST be a JSON object (not a string). For tables include \"rows\" (int) and \"columns\" (list of strings).\n\n\n    Implementation notes:\n    - Use pandas (and optionally polars) and Plotly. Set the Plotly default template to 'plotly_dark'.\n    - Load inputs from the artifact list. Prefer pd.read_parquet when a Parquet path is provided; otherwise use pd.read_csv.\n    - Persist all tabular outputs under ${DATA_DIR}.\n    - At the end, write 'result.json' with tables, figures, metrics, explanation as specified.\n    - Do not access any network resources; only read local artifacts.\n\n    EXAMPLES:\n    Example 1:\nDescription: This Python code conducts a rolling regression of Factor A on 1-day forward SPY returns using LangGraph state, enabling predictive analysis of factor performance. It computes residuals and calculates rolling percentile ranks to identify relative performance within rolling windows. Results are stored in state for subsequent agents to use in trading signal generation or risk assessment workflows.\nCode:\n```python\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.regression.rolling import RollingOLS\nimport statsmodels.api as sm\n\n# Tool call example - factor and SPY data would be available in state:\n# state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n# state['factors']['factor_a'] = some_calculated_factor_series\n\ndef perform_rolling_regression_and_percentile_rank(state, factor_key, spy_key='SPY_daily', rolling_window=250, rank_window=20):\n    # Access data from LangGraph state\n    if factor_key not in state.get('factors', {}) and factor_key not in state.get('dataframes', {}):\n        raise ValueError(f'Factor data ({factor_key}) not found in state.')\n    if spy_key not in state.get('dataframes', {}):\n        raise ValueError(f'SPY data ({spy_key}) not found in state.')\n\n    # Get factor and SPY data\n    if factor_key in state.get('factors', {}):\n        factor_data = state['factors'][factor_key]\n    else:\n        factor_df = state['dataframes'][factor_key]\n        factor_data = factor_df.iloc[:, 0] if isinstance(factor_df, pd.DataFrame) else factor_df\n\n    spy_data = state['dataframes'][spy_key]\n    # Calculate 1-day forward returns for SPY\n    forward_returns = spy_data['Close'].pct_change().shift(-1)\n\n    # Merge data on Date index\n    data = pd.concat([factor_data, forward_returns], axis=1)\n    data.columns = ['Factor_A', 'Forward_SPY_Returns']\n    data = data.dropna()\n\n    # Perform rolling OLS\n    model = RollingOLS(endog=data['Forward_SPY_Returns'], exog=sm.add_constant(data['Factor_A']), window=rolling_window)\n    fitted_model = model.fit()\n    data['Residuals'] = fitted_model.resids\n\n    # Calculate rolling percentile rank of residuals\n    data['Residuals_Percentile_Rank'] = data['Residuals'].rolling(window=rank_window).apply(\n        lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=False)\n\n    # Store results back in state\n    state.setdefault('factors', {})[f'{factor_key}_forward_regression_percentile'] = data['Residuals_Percentile_Rank']\n    state.setdefault('analysis_results', {})[f'{factor_key}_forward_regression'] = {\n        'residuals': data['Residuals'],\n        'percentile_ranks': data['Residuals_Percentile_Rank'],\n        'predictive_power': fitted_model.rsquared.mean() if hasattr(fitted_model.rsquared, 'mean') else None\n    }\n\n    return data[['Residuals_Percentile_Rank']]\n\n# Example usage in LangGraph agent:\n# 1. Extract data and calculate factor:\n#    state['dataframes']['SPY_daily'] = extract_daily_stock_data('SPY', '2020-01-01', '2023-12-31', '1d')\n#    state['factors']['momentum_factor'] = some_momentum_calculation\n# 2. Perform forward-looking regression analysis:\npercentile_ranks = perform_rolling_regression_and_percentile_rank(state, 'momentum_factor', rolling_window=250, rank_window=20)\nprint(percentile_ranks)\n\n```\n\nExample 2:\nDescription: This Python code computes the SMA-based momentum factor for a list of stocks using LangGraph state. It accesses stock data from the state dict, calculates the difference between 20-day and 200-day simple moving averages for each ticker, and stores results in state under both 'factors' and 'signals'. This momentum signal can be used by subsequent portfolio optimization or trading strategy agents in the LangGraph workflow.\nCode:\n```python\nimport pandas as pd\nimport numpy as np\n\n# Tool call example - ticker data would be extracted beforehand:\n# from tools_clean import bulk_extract_daily_closing_prices_from_polygon\n# tickers = ['AAPL', 'MSFT', 'GOOGL']\n# merged_data = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# state['dataframes']['merged_closing_prices'] = merged_data\n\ndef calculate_momentum_factor(state, tickers):\n    momentum_factors = {}\n    \n    for ticker in tickers:\n        # Try to access data from state - check multiple possible keys\n        stock_data = None\n        if 'merged_closing_prices' in state.get('dataframes', {}):\n            merged_df = state['dataframes']['merged_closing_prices']\n            if ticker in merged_df.columns:\n                stock_data = pd.DataFrame({ticker: merged_df[ticker]})\n                stock_data.columns = ['Close']\n        elif f'{ticker}_daily' in state.get('dataframes', {}):\n            stock_data = state['dataframes'][f'{ticker}_daily']\n        \n        if stock_data is None:\n            raise ValueError(f'Stock data for {ticker} not found in state. Ensure data extraction was performed first.')\n        \n        # Calculate SMA-based momentum factor\n        sma_20 = stock_data['Close'].rolling(window=20).mean()\n        sma_200 = stock_data['Close'].rolling(window=200).mean()\n        momentum_factors[ticker] = sma_20 - sma_200\n    \n    momentum_df = pd.DataFrame(momentum_factors)\n    \n    # Store results back in state\n    state.setdefault('factors', {})['sma_momentum'] = momentum_df\n    state.setdefault('signals', {})['sma_momentum_signals'] = momentum_df\n    \n    return momentum_df\n\n# Example usage in LangGraph agent:\n# 1. First extract ticker data:\n#    tickers = ['AAPL', 'MSFT', 'GOOGL']\n#    state['dataframes']['merged_closing_prices'] = bulk_extract_daily_closing_prices_from_polygon(tickers, '2020-01-01', '2023-12-31')\n# 2. Then calculate SMA momentum factor:\ntickers = ['AAPL', 'MSFT', 'GOOGL']\nmomentum_factor = calculate_momentum_factor(state, tickers)\nprint(momentum_factor)\n\n```\n\nExample 3:\nDescription: This Python script uses Plotly to perform a decile (quintile) analysis of Factor A against SPY's 1-day forward returns. It calculates rolling percentile ranks for Factor A over a 60-day window and bins these ranks into 5 equi-spaced percentiles. The script then merges these binned ranks with SPY's forward returns, computes the average return for each bin, and visualizes the results in a bar chart using Plotly's dark mode theme. This visualization helps to understand how different levels of Factor A influence SPY returns, providing a clear and interactive means to assess the factor's predictive power on stock market movements.\nCode:\n```python\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Load data function\ndef load_data(factor_path, spy_path):\n    factors = pd.read_csv(factor_path, parse_dates=['Date'], index_col='Date')\n    spy_prices = pd.read_csv(spy_path, parse_dates=['Date'], index_col='Date')\n    return factors, spy_prices\n\n# Prepare data function\ndef prepare_data(factors, spy_prices, num_bins):\n    spy_prices['Forward_Return'] = spy_prices['Close'].pct_change().shift(-1)\n    factors['Factor_A_Rank'] = factors['Factor A'].rolling(window=60).apply(lambda x: pd.qcut(x.rank(method='first'), num_bins, labels=range(1, num_bins + 1)))\n    combined_data = factors[['Factor_A_Rank']].join(spy_prices['Forward_Return']).dropna()\n    return combined_data\n\n# Analyze and plot function\ndef analyze_and_plot(data, num_bins):\n    grouped_data = data.groupby('Factor_A_Rank')['Forward_Return'].mean()\n    fig = go.Figure(data=[go.Bar(x=list(range(1, num_bins + 1)), y=grouped_data, marker_color='lightskyblue')])\n    fig.update_layout(\n        title='Decile Analysis of Factor A vs. SPY 1D Forward Returns',\n        xaxis_title='Decile of Factor A',\n        yaxis_title='Average SPY 1D Forward Returns',\n        template='plotly_dark'\n    )\n    fig.show()\n\n# Main execution\nfactor_path = '/path/to/factor.csv'\nspy_path = '/path/to/SPY_daily_ohlcv.csv'\nnum_bins = 5\nfactors, spy_prices = load_data(factor_path, spy_path)\ndata = prepare_data(factors, spy_prices, num_bins)\nanalyze_and_plot(data, num_scores)\n\n```\n\n    USER REQUEST: Is insider buying a leading signal? Rank stocks by net insider buys in the last 90 days and run a 3-month forward performance test.\n    SELECTED TOOLS: extract_latest_insider_trades, bulk_extract_daily_closing_prices_from_polygon, extract_fundamentals_from_fmp\n\n    INTERMEDIATE USAGE CONTEXT:\n    Plan:\n- insider_trades: Compute net insider buying by symbol over the last 90 days. Aggregate securitiesTransacted with + for A (acquisitions) and - for D (dispositions); optionally compute dollar net = shares * price. Rank symbols by net value.\n- Price/returns data: Needed to run 3-month forward performance test (not provided in available datasets). Join by symbol and event/portfolio-formation date to compute 3M forward returns.\n\nDataset: insider_trades \u2014 rank inputs (net insider buys over last 90 days)\nsymbol,transactionDate,acquisitionOrDisposition,securitiesTransacted,price,formType,directOrIndirect\nTMUS,2025-09-24,D,27107.0,237.5579,4,D\ncol_desc:\nsymbol: Ticker symbol\ntransactionDate: Insider transaction date (use for 90-day window)\nacquisitionOrDisposition: A=Acquisition (buy), D=Disposition (sell) for sign\nsecuritiesTransacted: Shares transacted; aggregate by symbol and sign\nprice: Per-share transaction price; compute dollar net buys\nformType: SEC filing type; filter to Form 4\ndirectOrIndirect: Ownership nature (D=direct, I=indirect); optional filter\n\n    AVAILABLE DATA ARTIFACTS (load these files in your code):\n    - insider_trades: /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet\n    ", "generated_code": "import os\nimport json\nimport math\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.templates.default = \"plotly_dark\"\n\n# -----------------------------\n# Configuration\n# -----------------------------\nDATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Provided local artifact path (Parquet)\nINSIDER_TRADES_PATH = \"/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet\"\n\n# Backtest parameters (only applied if prices are locally available; this script does NOT fetch any prices)\nLOOKBACK_DAYS = 90\nFORWARD_HORIZON_TRADING_DAYS = 63  # ~3 months\nTOP_QUANTILE = 0.1  # long top 10% by net insider dollar buys\nBOTTOM_QUANTILE = 0.1  # optional short bottom 10% by net insider dollar buys\nSLIPPAGE_BPS = 5.0\nCOMMISSION_BPS = 1.0\nLEVERAGE_CAP = 1.0  # 100% max gross leverage\nALLOW_SHORTS = False  # If True and prices available, enable long-short; otherwise long-only\n\n# -----------------------------\n# Utilities\n# -----------------------------\ndef load_artifact(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Artifact not found: {path}\")\n    if path.lower().endswith(\".parquet\"):\n        return pd.read_parquet(path)\n    elif path.lower().endswith(\".csv\"):\n        return pd.read_csv(path)\n    else:\n        raise ValueError(f\"Unsupported file format for: {path}\")\n\n\ndef _to_datetime(s: pd.Series) -> pd.Series:\n    try:\n        return pd.to_datetime(s)\n    except Exception:\n        return pd.to_datetime(s, errors='coerce')\n\n\ndef normal_cdf(x: float) -> float:\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n\n\ndef compute_psr(returns: pd.Series, sr_benchmark: float = 0.0, freq: int = 252) -> Optional[float]:\n    r = returns.dropna()\n    n = len(r)\n    if n < 2:\n        return None\n    mu = r.mean()\n    sigma = r.std(ddof=1)\n    if sigma == 0 or np.isnan(sigma):\n        return None\n    sr_hat = (mu / sigma) * math.sqrt(freq)\n    skew = (r.skew() if not np.isnan(r.skew()) else 0.0)\n    kurt = (r.kurtosis(fisher=False) if not np.isnan(r.kurtosis()) else 3.0)  # Pearson kurtosis\n    denom = math.sqrt(max(1e-12, 1 - skew * sr_hat + ((kurt - 1) / 4.0) * (sr_hat ** 2)))\n    z = (sr_hat - sr_benchmark) * math.sqrt(max(1, n - 1)) / denom\n    return normal_cdf(z)\n\n\ndef bootstrap_sharpe_ci(returns: pd.Series, n_boot: int = 2000, alpha: float = 0.05, freq: int = 252, random_state: Optional[int] = 42) -> Optional[Tuple[float, float]]:\n    r = returns.dropna().values\n    n = len(r)\n    if n < 2:\n        return None\n    rng = np.random.default_rng(random_state)\n    sharps = []\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, size=n)\n        sample = r[idx]\n        s = sample.std(ddof=1)\n        if s == 0:\n            continue\n        sharps.append((sample.mean() / s) * math.sqrt(freq))\n    if not sharps:\n        return None\n    lo = float(np.quantile(sharps, alpha / 2))\n    hi = float(np.quantile(sharps, 1 - alpha / 2))\n    return lo, hi\n\n\n# -----------------------------\n# Insider analytics\n# -----------------------------\n\ndef compute_net_insider_flows(insider_df: pd.DataFrame, formation_date: pd.Timestamp, lookback_days: int = 90) -> pd.DataFrame:\n    df = insider_df.copy()\n    # Normalize columns\n    if 'transactionDate' not in df.columns:\n        raise ValueError(\"Expected column 'transactionDate' in insider trades artifact.\")\n    df['transactionDate'] = _to_datetime(df['transactionDate']).dt.tz_localize(None)\n\n    # Filter Form 4 where possible\n    if 'formType' in df.columns:\n        df['formType'] = df['formType'].astype(str).str.upper()\n        df = df[df['formType'].str.contains('4', regex=False)]\n\n    # 90-day window up to formation_date (inclusive)\n    start_date = formation_date - timedelta(days=lookback_days)\n    df = df[(df['transactionDate'] >= start_date) & (df['transactionDate'] <= formation_date)]\n\n    if df.empty:\n        return pd.DataFrame(columns=[\n            'symbol', 'net_shares_90d', 'net_dollar_90d', 'tx_count', 'buy_count', 'sell_count', 'last_trade_date', 'rank', 'rank_pct'\n        ])\n\n    # Sign mapping: A=+1, D=-1\n    if 'acquisitionOrDisposition' not in df.columns:\n        raise ValueError(\"Expected column 'acquisitionOrDisposition' in insider trades artifact.\")\n    df['acqdisp'] = df['acquisitionOrDisposition'].astype(str).str.upper().str[0]\n    sign_map = {'A': 1.0, 'D': -1.0}\n    df['sign'] = df['acqdisp'].map(sign_map).fillna(0.0)\n\n    # Shares and price columns\n    if 'securitiesTransacted' not in df.columns:\n        raise ValueError(\"Expected column 'securitiesTransacted' in insider trades artifact.\")\n    df['shares'] = pd.to_numeric(df['securitiesTransacted'], errors='coerce').fillna(0.0)\n    price_col = 'price' if 'price' in df.columns else None\n    if price_col:\n        df['price'] = pd.to_numeric(df[price_col], errors='coerce').fillna(0.0)\n    else:\n        df['price'] = 0.0\n\n    # Aggregation by symbol\n    grp = df.groupby('symbol', as_index=False).agg(\n        net_shares_90d=('shares', lambda x: float((df.loc[x.index, 'sign'] * x).sum())),\n        net_dollar_90d=('shares', lambda x: float((df.loc[x.index, 'sign'] * x * df.loc[x.index, 'price']).sum())),\n        tx_count=('shares', 'count'),\n        buy_count=('acqdisp', lambda x: int((x == 'A').sum())),\n        sell_count=('acqdisp', lambda x: int((x == 'D').sum())),\n        last_trade_date=('transactionDate', 'max')\n    )\n\n    # Rank by net dollar buys desc\n    grp['rank'] = grp['net_dollar_90d'].rank(ascending=False, method='first')\n    grp = grp.sort_values(['net_dollar_90d', 'net_shares_90d'], ascending=[False, False]).reset_index(drop=True)\n    grp['rank'] = np.arange(1, len(grp) + 1)\n    grp['rank_pct'] = 1.0 - (grp['rank'] - 1) / max(1, len(grp) - 1)\n\n    return grp\n\n\ndef plot_top_net_buys(df_rank: pd.DataFrame, formation_date: pd.Timestamp, top_n: int = 20) -> str:\n    if df_rank.empty:\n        # Create an empty placeholder figure\n        fig = go.Figure()\n        fig.update_layout(title=f\"No insider trades in the last {LOOKBACK_DAYS} days\", xaxis_title=\"Symbol\", yaxis_title=\"Net Dollar (90d)\")\n        out_html = os.path.join(DATA_DIR, f\"top_net_insider_buys_{formation_date.date()}_empty.html\")\n        fig.write_html(out_html, include_plotlyjs='inline', full_html=True)\n        return out_html\n\n    top_df = df_rank.nlargest(top_n, 'net_dollar_90d').copy()\n    fig = go.Figure(\n        data=[go.Bar(x=top_df['symbol'], y=top_df['net_dollar_90d'], marker_color='lightskyblue')]\n    )\n    fig.update_layout(\n        title=f\"Top {min(top_n, len(top_df))} Net Insider Dollar Buys (Last {LOOKBACK_DAYS} Days) as of {formation_date.date()}\",\n        xaxis_title=\"Symbol\",\n        yaxis_title=\"Net Dollar Buy (shares * price)\",\n        bargap=0.2,\n    )\n    out_html = os.path.join(DATA_DIR, f\"top_net_insider_buys_{formation_date.strftime('%Y%m%d')}.html\")\n    fig.write_html(out_html, include_plotlyjs='inline', full_html=True)\n    return out_html\n\n\n# -----------------------------\n# Forward return backtest scaffolding (prices required; this run does not fetch any)\n# -----------------------------\n\ndef enforce_position_limits(weights: pd.Series, leverage_cap: float = 1.0) -> pd.Series:\n    w = weights.fillna(0.0)\n    gross = float(np.abs(w).sum())\n    if gross == 0:\n        return w\n    if gross > max(1e-12, leverage_cap):\n        w = w * (leverage_cap / gross)\n    return w\n\n\ndef backtest_forward_3m(prices: Dict[str, pd.DataFrame], formation_date: pd.Timestamp, long_symbols: List[str], short_symbols: Optional[List[str]] = None,\n                        horizon_days: int = FORWARD_HORIZON_TRADING_DAYS, slippage_bps: float = SLIPPAGE_BPS, commission_bps: float = COMMISSION_BPS,\n                        leverage_cap: float = LEVERAGE_CAP, allow_shorts: bool = ALLOW_SHORTS) -> Dict:\n    # This function requires prices dict with Date index and columns containing 'Adj Close' or 'Close' (and optionally 'Open').\n    # If unavailable, raise and handle upstream.\n    if prices is None or len(prices) == 0:\n        raise ValueError(\"No local price data provided.\")\n\n    # Determine available next bar for each symbol\n    def get_cols(df: pd.DataFrame) -> Tuple[str, str]:\n        cols = df.columns.str.lower()\n        close_col = 'adj close' if 'adj close' in cols else ('close' if 'close' in cols else None)\n        open_col = 'open' if 'open' in cols else None\n        if close_col is None:\n            raise ValueError(\"Price DataFrame must have 'Adj Close' or 'Close' column.\")\n        # Map back to original case\n        for c in df.columns:\n            if c.lower() == close_col:\n                close_col = c\n            if open_col and c.lower() == open_col:\n                open_col = c\n        return open_col, close_col\n\n    # Portfolio weights at formation (equal-weight within long and short buckets)\n    long_set = [s for s in long_symbols if s in prices]\n    short_set = [s for s in (short_symbols or []) if s in prices] if allow_shorts else []\n\n    n_long = len(long_set)\n    n_short = len(short_set)\n\n    if n_long == 0 and (not allow_shorts or n_short == 0):\n        raise ValueError(\"No symbols with available prices to backtest.\")\n\n    w_long = pd.Series(1.0 / n_long, index=long_set) if n_long > 0 else pd.Series(dtype=float)\n    w_short = pd.Series(-1.0 / n_short, index=short_set) if allow_shorts and n_short > 0 else pd.Series(dtype=float)\n    weights = pd.concat([w_long, w_short])\n    weights = enforce_position_limits(weights, leverage_cap=leverage_cap)\n\n    # One-bar delay: trade at t+1 open if available; otherwise t+1 close (conservative), apply slippage and commission\n    # Build daily portfolio returns over horizon\n    portfolio_daily = []\n    symbols_used = list(weights.index)\n\n    # Establish common date range: from next trading day after formation_date\n    # For each symbol, find index > formation_date\n    start_dates = []\n    end_dates = []\n    for s in symbols_used:\n        df = prices[s].copy()\n        df = df.sort_index()\n        df.index = pd.to_datetime(df.index).tz_localize(None)\n        # Ensure we have at least one bar after formation\n        next_idx = df.index[df.index > formation_date]\n        if len(next_idx) == 0:\n            continue\n        start_dates.append(next_idx[0])\n        if len(df.index) > 0:\n            # horizon_days bars after start\n            pos = df.index.get_indexer([next_idx[0]])[0]\n            end_pos = min(pos + horizon_days, len(df.index) - 1)\n            end_dates.append(df.index[end_pos])\n    if not start_dates or not end_dates:\n        raise ValueError(\"Insufficient price history post formation date for horizon.\")\n\n    common_start = max(start_dates)\n    common_end = min(end_dates)\n    if common_start >= common_end:\n        raise ValueError(\"No overlapping forward window across symbols.\")\n\n    date_index = pd.DatetimeIndex(pd.unique(np.sort(np.concatenate([prices[s].index.values for s in symbols_used if s in prices]))))\n    date_index = date_index[(date_index >= common_start) & (date_index <= common_end)]\n\n    # Build daily returns panel\n    ret_df = pd.DataFrame(index=date_index, columns=symbols_used, dtype=float)\n    for s in symbols_used:\n        df = prices[s].sort_index().copy()\n        df.index = pd.to_datetime(df.index).tz_localize(None)\n        open_col, close_col = get_cols(df)\n        # Compute trade price and daily returns\n        # Entry price at first date in date_index: at open if available; else close\n        if len(df.index[df.index == date_index[0]]) == 0:\n            continue\n        entry_px = df.loc[date_index[0], open_col] if open_col and not np.isnan(df.loc[date_index[0], open_col]) else df.loc[date_index[0], close_col]\n        # Daily close-to-close returns as proxy for PnL evolution after entry\n        px = df.reindex(date_index)[close_col].astype(float)\n        daily_ret = px.pct_change().fillna(0.0)\n        # Apply frictions on entry and exit days: slippage + commission\n        fric = (slippage_bps + commission_bps) / 10000.0\n        if len(daily_ret) > 0:\n            daily_ret.iloc[0] -= fric  # entry costs\n            if len(daily_ret) > 1:\n                daily_ret.iloc[-1] -= fric  # exit costs\n        ret_df[s] = daily_ret\n\n    ret_df = ret_df.dropna(how='all')\n    # Portfolio daily return\n    weights_aligned = weights.reindex(ret_df.columns).fillna(0.0)\n    port_daily = (ret_df * weights_aligned).sum(axis=1)\n\n    equity = (1.0 + port_daily).cumprod()\n    total_ret = float(equity.iloc[-1] - 1.0)\n    ann_factor = 252 / max(1, len(port_daily)) * len(port_daily)  # simplifies to 252\n    sharpe = None\n    psr = None\n    ci = None\n    if port_daily.std(ddof=1) > 0:\n        sharpe = float((port_daily.mean() / port_daily.std(ddof=1)) * math.sqrt(252))\n        psr = compute_psr(port_daily, sr_benchmark=0.0, freq=252)\n        ci = bootstrap_sharpe_ci(port_daily, n_boot=2000, alpha=0.05, freq=252)\n\n    turnover = float(np.abs(weights_aligned).sum())  # single rebalance\n    gross_exposure = float(np.abs(weights_aligned).sum())\n    net_exposure = float(weights_aligned.sum())\n\n    dd = (equity / equity.cummax() - 1.0)\n    max_dd = float(dd.min()) if len(dd) else None\n\n    # HHI of weights\n    hhi = float((weights_aligned ** 2).sum()) if len(weights_aligned) else None\n\n    return {\n        'daily_returns': port_daily,\n        'equity_curve': equity,\n        'total_return': total_ret,\n        'sharpe': sharpe,\n        'psr': psr,\n        'sharpe_ci_95': ci,\n        'turnover': turnover,\n        'gross_exposure': gross_exposure,\n        'net_exposure': net_exposure,\n        'max_drawdown': max_dd,\n        'hhi_weights': hhi,\n        'weights': weights_aligned.to_dict(),\n    }\n\n\n# -----------------------------\n# Main execution\n# -----------------------------\n\ndef main():\n    # Load insider trades artifact\n    insider = load_artifact(INSIDER_TRADES_PATH)\n    # Ensure required columns exist\n    required_cols = {'symbol', 'transactionDate', 'acquisitionOrDisposition', 'securitiesTransacted'}\n    missing = required_cols - set(insider.columns)\n    if missing:\n        raise ValueError(f\"Missing required columns in insider trades: {missing}\")\n\n    insider['transactionDate'] = _to_datetime(insider['transactionDate']).dt.tz_localize(None)\n    if insider['transactionDate'].isna().all():\n        raise ValueError(\"transactionDate could not be parsed to datetime.\")\n\n    formation_date = pd.to_datetime(insider['transactionDate'].max()).to_pydatetime()\n\n    # Compute net insider flows\n    rank_df = compute_net_insider_flows(insider, pd.Timestamp(formation_date), lookback_days=LOOKBACK_DAYS)\n\n    # Persist table\n    rank_out_path = os.path.join(DATA_DIR, f\"insider_net_90d_rank_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n    rank_df.to_csv(rank_out_path, index=False)\n\n    # Plot figures (bar of top N)\n    fig_top_path = plot_top_net_buys(rank_df, pd.Timestamp(formation_date), top_n=20)\n\n    # Attempt forward 3M test only if local prices are provided (not available in this run)\n    backtest_metrics = {\n        'status': 'skipped_missing_prices',\n        'reason': 'No local price artifacts provided to compute 3-month forward returns.',\n    }\n\n    # If you later supply local price artifacts, populate a dict like {symbol: price_df}\n    # prices = { 'AAPL': aapl_df, ... }\n    prices = None  # Placeholder; do not fetch any data\n\n    long_list = []\n    short_list = []\n    if not rank_df.empty:\n        # Determine long/short buckets by quantiles\n        n = len(rank_df)\n        k_long = max(1, int(math.floor(TOP_QUANTILE * n)))\n        k_short = max(1, int(math.floor(BOTTOM_QUANTILE * n))) if ALLOW_SHORTS else 0\n        long_list = rank_df.head(k_long)['symbol'].tolist()\n        if ALLOW_SHORTS:\n            short_list = rank_df.tail(k_short)['symbol'].tolist()\n\n    if prices is not None and long_list:\n        try:\n            bt = backtest_forward_3m(\n                prices=prices,\n                formation_date=pd.Timestamp(formation_date),\n                long_symbols=long_list,\n                short_symbols=short_list if ALLOW_SHORTS else None,\n                horizon_days=FORWARD_HORIZON_TRADING_DAYS,\n                slippage_bps=SLIPPAGE_BPS,\n                commission_bps=COMMISSION_BPS,\n                leverage_cap=LEVERAGE_CAP,\n                allow_shorts=ALLOW_SHORTS,\n            )\n            # Save daily returns and equity if available\n            daily_ret_path = os.path.join(DATA_DIR, f\"forward_3m_daily_returns_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n            bt['daily_returns'].to_csv(daily_ret_path, header=['daily_return'])\n            equity_path = os.path.join(DATA_DIR, f\"forward_3m_equity_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n            bt['equity_curve'].to_csv(equity_path, header=['equity'])\n\n            backtest_metrics = {\n                'status': 'completed',\n                'total_return': bt['total_return'],\n                'sharpe': bt['sharpe'],\n                'psr': bt['psr'],\n                'sharpe_ci_95': bt['sharpe_ci_95'],\n                'turnover': bt['turnover'],\n                'gross_exposure': bt['gross_exposure'],\n                'net_exposure': bt['net_exposure'],\n                'max_drawdown': bt['max_drawdown'],\n                'hhi_weights': bt['hhi_weights'],\n                'n_long': len(long_list),\n                'n_short': len(short_list) if ALLOW_SHORTS else 0,\n                'weights': bt['weights'],\n                'daily_returns_path': daily_ret_path,\n                'equity_curve_path': equity_path,\n            }\n        except Exception as e:\n            backtest_metrics = {\n                'status': 'error',\n                'error': str(e),\n            }\n\n    # Assemble result.json\n    tables = [\n        {\n            'name': 'insider_net_90d_rank',\n            'path': rank_out_path,\n            'rows': int(rank_df.shape[0]),\n            'columns': [str(c) for c in rank_df.columns],\n            'description': f\"Net insider buying over last {LOOKBACK_DAYS} days by symbol, ranked by net_dollar_90d\"\n        }\n    ]\n\n    figures = [\n        {\n            'name': 'top_net_dollar_buys_bar',\n            'path': fig_top_path,\n            'format': 'html',\n            'description': f\"Top 20 net dollar insider buyers over the last {LOOKBACK_DAYS} days\"\n        }\n    ]\n\n    metrics = {\n        'formation_date': pd.Timestamp(formation_date).strftime('%Y-%m-%d'),\n        'lookback_days': LOOKBACK_DAYS,\n        'universe_size_90d': int(rank_df.shape[0]),\n        'long_bucket_size': len(long_list),\n        'short_bucket_size': len(short_list) if ALLOW_SHORTS else 0,\n        'backtest': backtest_metrics,\n        'frictions_bps': {\n            'slippage': SLIPPAGE_BPS,\n            'commission': COMMISSION_BPS\n        },\n        'leverage_cap': LEVERAGE_CAP,\n        'allow_shorts': ALLOW_SHORTS,\n    }\n\n    explanation = (\n        \"This analysis ranks stocks by net insider buying (shares * price with A=+1, D=-1) over the last 90 days \"\n        f\"ending on {pd.Timestamp(formation_date).date()}. It saves a ranking table and a Plotly bar chart of the top names. \"\n        \"A 3-month forward performance backtest is prepared with strict no-lookahead (trade next bar), frictions (slippage and commissions), \"\n        \"position limits, and exposure caps; however, it is skipped in this run because no local price artifacts were provided. \"\n        \"If price data (with adjusted prices) are supplied locally, the script will compute forward returns, Sharpe, Probabilistic Sharpe Ratio, \"\n        \"95% bootstrap CI for Sharpe, turnover, gross/net exposure, max drawdown, and HHI of weights, and will save the equity and returns time series.\"\n    )\n\n    result = {\n        'tables': tables,\n        'figures': figures,\n        'metrics': metrics,\n        'explanation': explanation\n    }\n\n    with open('result.json', 'w') as f:\n        json.dump(result, f, indent=2)\n\n\nif __name__ == '__main__':\n    main()\n", "code_explanation": "Loads the provided insider trades Parquet, computes 90-day net insider buying per symbol (signing A=+ and D=\u2212), ranks by net dollar buys, saves the ranked table to ${DATA_DIR}, and produces a Plotly (dark theme) bar chart of the top names as a standalone HTML file. It includes a backtest scaffold for a 3-month forward test with no-lookahead, one-bar delay, frictions, leverage caps, and exposure controls; this run skips the backtest because no local price data artifacts were provided. Outputs are summarized in result.json with tables, figures, and metrics.", "execution_result": {"success": true, "stdout": "", "stderr": ""}, "error_message": "", "reflection_count": 0, "final_response": "Code executed successfully!\n\nLoads the provided insider trades Parquet, computes 90-day net insider buying per symbol (signing A=+ and D=\u2212), ranks by net dollar buys, saves the ranked table to ${DATA_DIR}, and produces a Plotly (dark theme) bar chart of the top names as a standalone HTML file. It includes a backtest scaffold for a 3-month forward test with no-lookahead, one-bar delay, frictions, leverage caps, and exposure controls; this run skips the backtest because no local price data artifacts were provided. Outputs are summarized in result.json with tables, figures, and metrics.\n\nOutput:\n\n\nArtifacts available in: /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0\n\nWarnings: Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet, Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet, Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet, Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "intermediate_results": {}, "tool_params": {}, "tool_results": {"insider_trades": {"ok": true, "file_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "rows": 500, "columns": 16, "dtypes": {"symbol": "object", "filingDate": "object", "transactionDate": "object", "reportingCik": "object", "companyCik": "object", "transactionType": "object", "securitiesOwned": "float64", "reportingName": "object", "typeOfOwner": "object", "acquisitionOrDisposition": "object", "directOrIndirect": "object", "formType": "object", "securitiesTransacted": "float64", "price": "float64", "securityName": "object", "url": "object"}, "nulls": {"symbol": 0, "filingDate": 0, "transactionDate": 0, "reportingCik": 0, "companyCik": 0, "transactionType": 0, "securitiesOwned": 0, "reportingName": 0, "typeOfOwner": 0, "acquisitionOrDisposition": 0, "directOrIndirect": 45, "formType": 0, "securitiesTransacted": 0, "price": 0, "securityName": 0, "url": 0}, "date_start": null, "date_end": null, "sample_head": [{"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634407663.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 27107.0, "price": 237.5579, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634381662.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 26001.0, "price": 238.322, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634377397.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 4265.0, "price": 239.4642, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634373912.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 3485.0, "price": 240.5735, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634371050.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 2862.0, "price": 241.4435, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}]}}, "tool_files": {"insider_trades": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet"}, "data_dir": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974", "tool_calls_log": [{"tool": "extract_latest_insider_trades", "semantic_key": "insider_trades", "args": {"start_date": "2022-01-01", "end_date": "2025-09-26"}, "param_issues": []}, {"tool": "extract_latest_insider_trades", "semantic_key": "insider_trades", "args": {"start_date": "2025-06-28", "end_date": "2025-09-26"}, "param_issues": []}, {"tool": "extract_latest_insider_trades", "semantic_key": "insider_trades", "args": {"start_date": "2020-01-01", "end_date": "2025-06-26"}, "param_issues": []}], "tool_outputs": {"insider_trades": {"ok": true, "file_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "rows": 500, "columns": 16, "dtypes": {"symbol": "object", "filingDate": "object", "transactionDate": "object", "reportingCik": "object", "companyCik": "object", "transactionType": "object", "securitiesOwned": "float64", "reportingName": "object", "typeOfOwner": "object", "acquisitionOrDisposition": "object", "directOrIndirect": "object", "formType": "object", "securitiesTransacted": "float64", "price": "float64", "securityName": "object", "url": "object"}, "nulls": {"symbol": 0, "filingDate": 0, "transactionDate": 0, "reportingCik": 0, "companyCik": 0, "transactionType": 0, "securitiesOwned": 0, "reportingName": 0, "typeOfOwner": 0, "acquisitionOrDisposition": 0, "directOrIndirect": 45, "formType": 0, "securitiesTransacted": 0, "price": 0, "securityName": 0, "url": 0}, "date_start": null, "date_end": null, "sample_head": [{"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634407663.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 27107.0, "price": 237.5579, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634381662.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 26001.0, "price": 238.322, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634377397.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 4265.0, "price": 239.4642, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634373912.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 3485.0, "price": 240.5735, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634371050.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 2862.0, "price": 241.4435, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}]}}, "intermediate_context": "Plan:\n- insider_trades: Compute net insider buying by symbol over the last 90 days. Aggregate securitiesTransacted with + for A (acquisitions) and - for D (dispositions); optionally compute dollar net = shares * price. Rank symbols by net value.\n- Price/returns data: Needed to run 3-month forward performance test (not provided in available datasets). Join by symbol and event/portfolio-formation date to compute 3M forward returns.\n\nDataset: insider_trades \u2014 rank inputs (net insider buys over last 90 days)\nsymbol,transactionDate,acquisitionOrDisposition,securitiesTransacted,price,formType,directOrIndirect\nTMUS,2025-09-24,D,27107.0,237.5579,4,D\ncol_desc:\nsymbol: Ticker symbol\ntransactionDate: Insider transaction date (use for 90-day window)\nacquisitionOrDisposition: A=Acquisition (buy), D=Disposition (sell) for sign\nsecuritiesTransacted: Shares transacted; aggregate by symbol and sign\nprice: Per-share transaction price; compute dollar net buys\nformType: SEC filing type; filter to Form 4\ndirectOrIndirect: Ownership nature (D=direct, I=indirect); optional filter", "run_id": "6addf21b-f3f3-43bf-9c8a-d7b0e11a2974", "warnings": ["Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet"], "tool_artifacts": [{"tool": "extract_latest_insider_trades", "params": {"start_date": "2022-01-01", "end_date": "2025-09-26"}, "sig": "03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992", "sem_key": "insider_trades", "artifact_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet", "from_cache": true, "rows": 1, "cols": 1}, {"tool": "extract_latest_insider_trades", "params": {"start_date": "2025-06-28", "end_date": "2025-09-26"}, "sig": "16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2", "sem_key": "insider_trades", "artifact_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "from_cache": true, "rows": 1, "cols": 1}, {"tool": "extract_latest_insider_trades", "params": {"start_date": "2020-01-01", "end_date": "2025-06-26"}, "sig": "95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377", "sem_key": "insider_trades", "artifact_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "from_cache": false, "rows": 500, "cols": 16}], "base_data_dir": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z", "result_manifest": "ResultManifest(tables=[TableArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/insider_net_90d_rank_20250926.csv', rows=102, columns=['symbol', 'net_shares_90d', 'net_dollar_90d', 'tx_count', 'buy_count', 'sell_count', 'last_trade_date', 'rank', 'rank_pct'], schema=None, description=None)], figures=[FigureArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/top_net_insider_buys_20250926.html', caption=None, format='html')], metrics={'formation_date': '2025-09-26', 'lookback_days': 90, 'universe_size_90d': 102, 'long_bucket_size': 10, 'short_bucket_size': 0, 'backtest': {'status': 'skipped_missing_prices', 'reason': 'No local price artifacts provided to compute 3-month forward returns.'}, 'frictions_bps': {'slippage': 5.0, 'commission': 1.0}, 'leverage_cap': 1.0, 'allow_shorts': False}, explanation='This analysis ranks stocks by net insider buying (shares * price with A=+1, D=-1) over the last 90 days ending on 2025-09-26. It saves a ranking table and a Plotly bar chart of the top names. A 3-month forward performance backtest is prepared with strict no-lookahead (trade next bar), frictions (slippage and commissions), position limits, and exposure caps; however, it is skipped in this run because no local price artifacts were provided. If price data (with adjusted prices) are supplied locally, the script will compute forward returns, Sharpe, Probabilistic Sharpe Ratio, 95% bootstrap CI for Sharpe, turnover, gross/net exposure, max drawdown, and HHI of weights, and will save the equity and returns time series.')", "iteration_workdirs": ["/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0"], "final_manifest_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/result.json", "exec_workdir": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0", "manifest_ok": true, "iteration_manifests": ["ResultManifest(tables=[TableArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/insider_net_90d_rank_20250926.csv', rows=102, columns=['symbol', 'net_shares_90d', 'net_dollar_90d', 'tx_count', 'buy_count', 'sell_count', 'last_trade_date', 'rank', 'rank_pct'], schema=None, description=None)], figures=[FigureArtifact(path='/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/top_net_insider_buys_20250926.html', caption=None, format='html')], metrics={'formation_date': '2025-09-26', 'lookback_days': 90, 'universe_size_90d': 102, 'long_bucket_size': 10, 'short_bucket_size': 0, 'backtest': {'status': 'skipped_missing_prices', 'reason': 'No local price artifacts provided to compute 3-month forward returns.'}, 'frictions_bps': {'slippage': 5.0, 'commission': 1.0}, 'leverage_cap': 1.0, 'allow_shorts': False}, explanation='This analysis ranks stocks by net insider buying (shares * price with A=+1, D=-1) over the last 90 days ending on 2025-09-26. It saves a ranking table and a Plotly bar chart of the top names. A 3-month forward performance backtest is prepared with strict no-lookahead (trade next bar), frictions (slippage and commissions), position limits, and exposure caps; however, it is skipped in this run because no local price artifacts were provided. If price data (with adjusted prices) are supplied locally, the script will compute forward returns, Sharpe, Probabilistic Sharpe Ratio, 95% bootstrap CI for Sharpe, turnover, gross/net exposure, max drawdown, and HHI of weights, and will save the equity and returns time series.')"], "iteration_index": 0, "planning_turn": 3, "plan_history": [[{"tool": "extract_latest_insider_trades", "params": {"start_date": "2022-01-01", "end_date": "2025-09-26"}, "sig": "03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992"}], [{"tool": "extract_latest_insider_trades", "params": {"start_date": "2025-06-28", "end_date": "2025-09-26"}, "sig": "16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2"}], [{"tool": "extract_latest_insider_trades", "params": {"start_date": "2020-01-01", "end_date": "2025-06-26"}, "sig": "95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377"}]], "assumptions": [], "stop_reason": "budget", "eval_report": {"improvements": [], "research_goal": "(fallback)", "research_queries": [], "new_tool_suggestions": [], "notes": "(fallback)"}, "tavily_results": {"goal": "(fallback)", "queries": [], "evidences": [], "recommended_actions": []}, "actions_synthesis": [], "actions_synthesis_all": [], "delta_plan": {}, "eval_bundle": {}, "delta_tool_outputs": {}, "improvement_round": 0, "should_finalize_after_eval": true, "acceptance_result": {"requested_new_columns": [], "new_columns_present": 0, "requested_metrics": [], "metrics_present": 0, "passed": true}, "acceptance_failed": false, "last_good_code": "import os\nimport json\nimport math\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\npio.templates.default = \"plotly_dark\"\n\n# -----------------------------\n# Configuration\n# -----------------------------\nDATA_DIR = os.environ.get(\"DATA_DIR\", \"./data\")\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Provided local artifact path (Parquet)\nINSIDER_TRADES_PATH = \"/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet\"\n\n# Backtest parameters (only applied if prices are locally available; this script does NOT fetch any prices)\nLOOKBACK_DAYS = 90\nFORWARD_HORIZON_TRADING_DAYS = 63  # ~3 months\nTOP_QUANTILE = 0.1  # long top 10% by net insider dollar buys\nBOTTOM_QUANTILE = 0.1  # optional short bottom 10% by net insider dollar buys\nSLIPPAGE_BPS = 5.0\nCOMMISSION_BPS = 1.0\nLEVERAGE_CAP = 1.0  # 100% max gross leverage\nALLOW_SHORTS = False  # If True and prices available, enable long-short; otherwise long-only\n\n# -----------------------------\n# Utilities\n# -----------------------------\ndef load_artifact(path: str) -> pd.DataFrame:\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Artifact not found: {path}\")\n    if path.lower().endswith(\".parquet\"):\n        return pd.read_parquet(path)\n    elif path.lower().endswith(\".csv\"):\n        return pd.read_csv(path)\n    else:\n        raise ValueError(f\"Unsupported file format for: {path}\")\n\n\ndef _to_datetime(s: pd.Series) -> pd.Series:\n    try:\n        return pd.to_datetime(s)\n    except Exception:\n        return pd.to_datetime(s, errors='coerce')\n\n\ndef normal_cdf(x: float) -> float:\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n\n\ndef compute_psr(returns: pd.Series, sr_benchmark: float = 0.0, freq: int = 252) -> Optional[float]:\n    r = returns.dropna()\n    n = len(r)\n    if n < 2:\n        return None\n    mu = r.mean()\n    sigma = r.std(ddof=1)\n    if sigma == 0 or np.isnan(sigma):\n        return None\n    sr_hat = (mu / sigma) * math.sqrt(freq)\n    skew = (r.skew() if not np.isnan(r.skew()) else 0.0)\n    kurt = (r.kurtosis(fisher=False) if not np.isnan(r.kurtosis()) else 3.0)  # Pearson kurtosis\n    denom = math.sqrt(max(1e-12, 1 - skew * sr_hat + ((kurt - 1) / 4.0) * (sr_hat ** 2)))\n    z = (sr_hat - sr_benchmark) * math.sqrt(max(1, n - 1)) / denom\n    return normal_cdf(z)\n\n\ndef bootstrap_sharpe_ci(returns: pd.Series, n_boot: int = 2000, alpha: float = 0.05, freq: int = 252, random_state: Optional[int] = 42) -> Optional[Tuple[float, float]]:\n    r = returns.dropna().values\n    n = len(r)\n    if n < 2:\n        return None\n    rng = np.random.default_rng(random_state)\n    sharps = []\n    for _ in range(n_boot):\n        idx = rng.integers(0, n, size=n)\n        sample = r[idx]\n        s = sample.std(ddof=1)\n        if s == 0:\n            continue\n        sharps.append((sample.mean() / s) * math.sqrt(freq))\n    if not sharps:\n        return None\n    lo = float(np.quantile(sharps, alpha / 2))\n    hi = float(np.quantile(sharps, 1 - alpha / 2))\n    return lo, hi\n\n\n# -----------------------------\n# Insider analytics\n# -----------------------------\n\ndef compute_net_insider_flows(insider_df: pd.DataFrame, formation_date: pd.Timestamp, lookback_days: int = 90) -> pd.DataFrame:\n    df = insider_df.copy()\n    # Normalize columns\n    if 'transactionDate' not in df.columns:\n        raise ValueError(\"Expected column 'transactionDate' in insider trades artifact.\")\n    df['transactionDate'] = _to_datetime(df['transactionDate']).dt.tz_localize(None)\n\n    # Filter Form 4 where possible\n    if 'formType' in df.columns:\n        df['formType'] = df['formType'].astype(str).str.upper()\n        df = df[df['formType'].str.contains('4', regex=False)]\n\n    # 90-day window up to formation_date (inclusive)\n    start_date = formation_date - timedelta(days=lookback_days)\n    df = df[(df['transactionDate'] >= start_date) & (df['transactionDate'] <= formation_date)]\n\n    if df.empty:\n        return pd.DataFrame(columns=[\n            'symbol', 'net_shares_90d', 'net_dollar_90d', 'tx_count', 'buy_count', 'sell_count', 'last_trade_date', 'rank', 'rank_pct'\n        ])\n\n    # Sign mapping: A=+1, D=-1\n    if 'acquisitionOrDisposition' not in df.columns:\n        raise ValueError(\"Expected column 'acquisitionOrDisposition' in insider trades artifact.\")\n    df['acqdisp'] = df['acquisitionOrDisposition'].astype(str).str.upper().str[0]\n    sign_map = {'A': 1.0, 'D': -1.0}\n    df['sign'] = df['acqdisp'].map(sign_map).fillna(0.0)\n\n    # Shares and price columns\n    if 'securitiesTransacted' not in df.columns:\n        raise ValueError(\"Expected column 'securitiesTransacted' in insider trades artifact.\")\n    df['shares'] = pd.to_numeric(df['securitiesTransacted'], errors='coerce').fillna(0.0)\n    price_col = 'price' if 'price' in df.columns else None\n    if price_col:\n        df['price'] = pd.to_numeric(df[price_col], errors='coerce').fillna(0.0)\n    else:\n        df['price'] = 0.0\n\n    # Aggregation by symbol\n    grp = df.groupby('symbol', as_index=False).agg(\n        net_shares_90d=('shares', lambda x: float((df.loc[x.index, 'sign'] * x).sum())),\n        net_dollar_90d=('shares', lambda x: float((df.loc[x.index, 'sign'] * x * df.loc[x.index, 'price']).sum())),\n        tx_count=('shares', 'count'),\n        buy_count=('acqdisp', lambda x: int((x == 'A').sum())),\n        sell_count=('acqdisp', lambda x: int((x == 'D').sum())),\n        last_trade_date=('transactionDate', 'max')\n    )\n\n    # Rank by net dollar buys desc\n    grp['rank'] = grp['net_dollar_90d'].rank(ascending=False, method='first')\n    grp = grp.sort_values(['net_dollar_90d', 'net_shares_90d'], ascending=[False, False]).reset_index(drop=True)\n    grp['rank'] = np.arange(1, len(grp) + 1)\n    grp['rank_pct'] = 1.0 - (grp['rank'] - 1) / max(1, len(grp) - 1)\n\n    return grp\n\n\ndef plot_top_net_buys(df_rank: pd.DataFrame, formation_date: pd.Timestamp, top_n: int = 20) -> str:\n    if df_rank.empty:\n        # Create an empty placeholder figure\n        fig = go.Figure()\n        fig.update_layout(title=f\"No insider trades in the last {LOOKBACK_DAYS} days\", xaxis_title=\"Symbol\", yaxis_title=\"Net Dollar (90d)\")\n        out_html = os.path.join(DATA_DIR, f\"top_net_insider_buys_{formation_date.date()}_empty.html\")\n        fig.write_html(out_html, include_plotlyjs='inline', full_html=True)\n        return out_html\n\n    top_df = df_rank.nlargest(top_n, 'net_dollar_90d').copy()\n    fig = go.Figure(\n        data=[go.Bar(x=top_df['symbol'], y=top_df['net_dollar_90d'], marker_color='lightskyblue')]\n    )\n    fig.update_layout(\n        title=f\"Top {min(top_n, len(top_df))} Net Insider Dollar Buys (Last {LOOKBACK_DAYS} Days) as of {formation_date.date()}\",\n        xaxis_title=\"Symbol\",\n        yaxis_title=\"Net Dollar Buy (shares * price)\",\n        bargap=0.2,\n    )\n    out_html = os.path.join(DATA_DIR, f\"top_net_insider_buys_{formation_date.strftime('%Y%m%d')}.html\")\n    fig.write_html(out_html, include_plotlyjs='inline', full_html=True)\n    return out_html\n\n\n# -----------------------------\n# Forward return backtest scaffolding (prices required; this run does not fetch any)\n# -----------------------------\n\ndef enforce_position_limits(weights: pd.Series, leverage_cap: float = 1.0) -> pd.Series:\n    w = weights.fillna(0.0)\n    gross = float(np.abs(w).sum())\n    if gross == 0:\n        return w\n    if gross > max(1e-12, leverage_cap):\n        w = w * (leverage_cap / gross)\n    return w\n\n\ndef backtest_forward_3m(prices: Dict[str, pd.DataFrame], formation_date: pd.Timestamp, long_symbols: List[str], short_symbols: Optional[List[str]] = None,\n                        horizon_days: int = FORWARD_HORIZON_TRADING_DAYS, slippage_bps: float = SLIPPAGE_BPS, commission_bps: float = COMMISSION_BPS,\n                        leverage_cap: float = LEVERAGE_CAP, allow_shorts: bool = ALLOW_SHORTS) -> Dict:\n    # This function requires prices dict with Date index and columns containing 'Adj Close' or 'Close' (and optionally 'Open').\n    # If unavailable, raise and handle upstream.\n    if prices is None or len(prices) == 0:\n        raise ValueError(\"No local price data provided.\")\n\n    # Determine available next bar for each symbol\n    def get_cols(df: pd.DataFrame) -> Tuple[str, str]:\n        cols = df.columns.str.lower()\n        close_col = 'adj close' if 'adj close' in cols else ('close' if 'close' in cols else None)\n        open_col = 'open' if 'open' in cols else None\n        if close_col is None:\n            raise ValueError(\"Price DataFrame must have 'Adj Close' or 'Close' column.\")\n        # Map back to original case\n        for c in df.columns:\n            if c.lower() == close_col:\n                close_col = c\n            if open_col and c.lower() == open_col:\n                open_col = c\n        return open_col, close_col\n\n    # Portfolio weights at formation (equal-weight within long and short buckets)\n    long_set = [s for s in long_symbols if s in prices]\n    short_set = [s for s in (short_symbols or []) if s in prices] if allow_shorts else []\n\n    n_long = len(long_set)\n    n_short = len(short_set)\n\n    if n_long == 0 and (not allow_shorts or n_short == 0):\n        raise ValueError(\"No symbols with available prices to backtest.\")\n\n    w_long = pd.Series(1.0 / n_long, index=long_set) if n_long > 0 else pd.Series(dtype=float)\n    w_short = pd.Series(-1.0 / n_short, index=short_set) if allow_shorts and n_short > 0 else pd.Series(dtype=float)\n    weights = pd.concat([w_long, w_short])\n    weights = enforce_position_limits(weights, leverage_cap=leverage_cap)\n\n    # One-bar delay: trade at t+1 open if available; otherwise t+1 close (conservative), apply slippage and commission\n    # Build daily portfolio returns over horizon\n    portfolio_daily = []\n    symbols_used = list(weights.index)\n\n    # Establish common date range: from next trading day after formation_date\n    # For each symbol, find index > formation_date\n    start_dates = []\n    end_dates = []\n    for s in symbols_used:\n        df = prices[s].copy()\n        df = df.sort_index()\n        df.index = pd.to_datetime(df.index).tz_localize(None)\n        # Ensure we have at least one bar after formation\n        next_idx = df.index[df.index > formation_date]\n        if len(next_idx) == 0:\n            continue\n        start_dates.append(next_idx[0])\n        if len(df.index) > 0:\n            # horizon_days bars after start\n            pos = df.index.get_indexer([next_idx[0]])[0]\n            end_pos = min(pos + horizon_days, len(df.index) - 1)\n            end_dates.append(df.index[end_pos])\n    if not start_dates or not end_dates:\n        raise ValueError(\"Insufficient price history post formation date for horizon.\")\n\n    common_start = max(start_dates)\n    common_end = min(end_dates)\n    if common_start >= common_end:\n        raise ValueError(\"No overlapping forward window across symbols.\")\n\n    date_index = pd.DatetimeIndex(pd.unique(np.sort(np.concatenate([prices[s].index.values for s in symbols_used if s in prices]))))\n    date_index = date_index[(date_index >= common_start) & (date_index <= common_end)]\n\n    # Build daily returns panel\n    ret_df = pd.DataFrame(index=date_index, columns=symbols_used, dtype=float)\n    for s in symbols_used:\n        df = prices[s].sort_index().copy()\n        df.index = pd.to_datetime(df.index).tz_localize(None)\n        open_col, close_col = get_cols(df)\n        # Compute trade price and daily returns\n        # Entry price at first date in date_index: at open if available; else close\n        if len(df.index[df.index == date_index[0]]) == 0:\n            continue\n        entry_px = df.loc[date_index[0], open_col] if open_col and not np.isnan(df.loc[date_index[0], open_col]) else df.loc[date_index[0], close_col]\n        # Daily close-to-close returns as proxy for PnL evolution after entry\n        px = df.reindex(date_index)[close_col].astype(float)\n        daily_ret = px.pct_change().fillna(0.0)\n        # Apply frictions on entry and exit days: slippage + commission\n        fric = (slippage_bps + commission_bps) / 10000.0\n        if len(daily_ret) > 0:\n            daily_ret.iloc[0] -= fric  # entry costs\n            if len(daily_ret) > 1:\n                daily_ret.iloc[-1] -= fric  # exit costs\n        ret_df[s] = daily_ret\n\n    ret_df = ret_df.dropna(how='all')\n    # Portfolio daily return\n    weights_aligned = weights.reindex(ret_df.columns).fillna(0.0)\n    port_daily = (ret_df * weights_aligned).sum(axis=1)\n\n    equity = (1.0 + port_daily).cumprod()\n    total_ret = float(equity.iloc[-1] - 1.0)\n    ann_factor = 252 / max(1, len(port_daily)) * len(port_daily)  # simplifies to 252\n    sharpe = None\n    psr = None\n    ci = None\n    if port_daily.std(ddof=1) > 0:\n        sharpe = float((port_daily.mean() / port_daily.std(ddof=1)) * math.sqrt(252))\n        psr = compute_psr(port_daily, sr_benchmark=0.0, freq=252)\n        ci = bootstrap_sharpe_ci(port_daily, n_boot=2000, alpha=0.05, freq=252)\n\n    turnover = float(np.abs(weights_aligned).sum())  # single rebalance\n    gross_exposure = float(np.abs(weights_aligned).sum())\n    net_exposure = float(weights_aligned.sum())\n\n    dd = (equity / equity.cummax() - 1.0)\n    max_dd = float(dd.min()) if len(dd) else None\n\n    # HHI of weights\n    hhi = float((weights_aligned ** 2).sum()) if len(weights_aligned) else None\n\n    return {\n        'daily_returns': port_daily,\n        'equity_curve': equity,\n        'total_return': total_ret,\n        'sharpe': sharpe,\n        'psr': psr,\n        'sharpe_ci_95': ci,\n        'turnover': turnover,\n        'gross_exposure': gross_exposure,\n        'net_exposure': net_exposure,\n        'max_drawdown': max_dd,\n        'hhi_weights': hhi,\n        'weights': weights_aligned.to_dict(),\n    }\n\n\n# -----------------------------\n# Main execution\n# -----------------------------\n\ndef main():\n    # Load insider trades artifact\n    insider = load_artifact(INSIDER_TRADES_PATH)\n    # Ensure required columns exist\n    required_cols = {'symbol', 'transactionDate', 'acquisitionOrDisposition', 'securitiesTransacted'}\n    missing = required_cols - set(insider.columns)\n    if missing:\n        raise ValueError(f\"Missing required columns in insider trades: {missing}\")\n\n    insider['transactionDate'] = _to_datetime(insider['transactionDate']).dt.tz_localize(None)\n    if insider['transactionDate'].isna().all():\n        raise ValueError(\"transactionDate could not be parsed to datetime.\")\n\n    formation_date = pd.to_datetime(insider['transactionDate'].max()).to_pydatetime()\n\n    # Compute net insider flows\n    rank_df = compute_net_insider_flows(insider, pd.Timestamp(formation_date), lookback_days=LOOKBACK_DAYS)\n\n    # Persist table\n    rank_out_path = os.path.join(DATA_DIR, f\"insider_net_90d_rank_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n    rank_df.to_csv(rank_out_path, index=False)\n\n    # Plot figures (bar of top N)\n    fig_top_path = plot_top_net_buys(rank_df, pd.Timestamp(formation_date), top_n=20)\n\n    # Attempt forward 3M test only if local prices are provided (not available in this run)\n    backtest_metrics = {\n        'status': 'skipped_missing_prices',\n        'reason': 'No local price artifacts provided to compute 3-month forward returns.',\n    }\n\n    # If you later supply local price artifacts, populate a dict like {symbol: price_df}\n    # prices = { 'AAPL': aapl_df, ... }\n    prices = None  # Placeholder; do not fetch any data\n\n    long_list = []\n    short_list = []\n    if not rank_df.empty:\n        # Determine long/short buckets by quantiles\n        n = len(rank_df)\n        k_long = max(1, int(math.floor(TOP_QUANTILE * n)))\n        k_short = max(1, int(math.floor(BOTTOM_QUANTILE * n))) if ALLOW_SHORTS else 0\n        long_list = rank_df.head(k_long)['symbol'].tolist()\n        if ALLOW_SHORTS:\n            short_list = rank_df.tail(k_short)['symbol'].tolist()\n\n    if prices is not None and long_list:\n        try:\n            bt = backtest_forward_3m(\n                prices=prices,\n                formation_date=pd.Timestamp(formation_date),\n                long_symbols=long_list,\n                short_symbols=short_list if ALLOW_SHORTS else None,\n                horizon_days=FORWARD_HORIZON_TRADING_DAYS,\n                slippage_bps=SLIPPAGE_BPS,\n                commission_bps=COMMISSION_BPS,\n                leverage_cap=LEVERAGE_CAP,\n                allow_shorts=ALLOW_SHORTS,\n            )\n            # Save daily returns and equity if available\n            daily_ret_path = os.path.join(DATA_DIR, f\"forward_3m_daily_returns_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n            bt['daily_returns'].to_csv(daily_ret_path, header=['daily_return'])\n            equity_path = os.path.join(DATA_DIR, f\"forward_3m_equity_{pd.Timestamp(formation_date).strftime('%Y%m%d')}.csv\")\n            bt['equity_curve'].to_csv(equity_path, header=['equity'])\n\n            backtest_metrics = {\n                'status': 'completed',\n                'total_return': bt['total_return'],\n                'sharpe': bt['sharpe'],\n                'psr': bt['psr'],\n                'sharpe_ci_95': bt['sharpe_ci_95'],\n                'turnover': bt['turnover'],\n                'gross_exposure': bt['gross_exposure'],\n                'net_exposure': bt['net_exposure'],\n                'max_drawdown': bt['max_drawdown'],\n                'hhi_weights': bt['hhi_weights'],\n                'n_long': len(long_list),\n                'n_short': len(short_list) if ALLOW_SHORTS else 0,\n                'weights': bt['weights'],\n                'daily_returns_path': daily_ret_path,\n                'equity_curve_path': equity_path,\n            }\n        except Exception as e:\n            backtest_metrics = {\n                'status': 'error',\n                'error': str(e),\n            }\n\n    # Assemble result.json\n    tables = [\n        {\n            'name': 'insider_net_90d_rank',\n            'path': rank_out_path,\n            'rows': int(rank_df.shape[0]),\n            'columns': [str(c) for c in rank_df.columns],\n            'description': f\"Net insider buying over last {LOOKBACK_DAYS} days by symbol, ranked by net_dollar_90d\"\n        }\n    ]\n\n    figures = [\n        {\n            'name': 'top_net_dollar_buys_bar',\n            'path': fig_top_path,\n            'format': 'html',\n            'description': f\"Top 20 net dollar insider buyers over the last {LOOKBACK_DAYS} days\"\n        }\n    ]\n\n    metrics = {\n        'formation_date': pd.Timestamp(formation_date).strftime('%Y-%m-%d'),\n        'lookback_days': LOOKBACK_DAYS,\n        'universe_size_90d': int(rank_df.shape[0]),\n        'long_bucket_size': len(long_list),\n        'short_bucket_size': len(short_list) if ALLOW_SHORTS else 0,\n        'backtest': backtest_metrics,\n        'frictions_bps': {\n            'slippage': SLIPPAGE_BPS,\n            'commission': COMMISSION_BPS\n        },\n        'leverage_cap': LEVERAGE_CAP,\n        'allow_shorts': ALLOW_SHORTS,\n    }\n\n    explanation = (\n        \"This analysis ranks stocks by net insider buying (shares * price with A=+1, D=-1) over the last 90 days \"\n        f\"ending on {pd.Timestamp(formation_date).date()}. It saves a ranking table and a Plotly bar chart of the top names. \"\n        \"A 3-month forward performance backtest is prepared with strict no-lookahead (trade next bar), frictions (slippage and commissions), \"\n        \"position limits, and exposure caps; however, it is skipped in this run because no local price artifacts were provided. \"\n        \"If price data (with adjusted prices) are supplied locally, the script will compute forward returns, Sharpe, Probabilistic Sharpe Ratio, \"\n        \"95% bootstrap CI for Sharpe, turnover, gross/net exposure, max drawdown, and HHI of weights, and will save the equity and returns time series.\"\n    )\n\n    result = {\n        'tables': tables,\n        'figures': figures,\n        'metrics': metrics,\n        'explanation': explanation\n    }\n\n    with open('result.json', 'w') as f:\n        json.dump(result, f, indent=2)\n\n\nif __name__ == '__main__':\n    main()\n", "last_good_manifest_summary": {"iteration": 0, "coercion_counts": {}, "table_count": 1, "figure_count": 1, "tables": [{"path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/insider_net_90d_rank_20250926.csv", "rows": 102, "columns": ["symbol", "net_shares_90d", "net_dollar_90d", "tx_count", "buy_count", "sell_count", "last_trade_date", "rank", "rank_pct"], "n_columns": 9}], "figures": [{"path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/top_net_insider_buys_20250926.html", "format": "html", "caption": null}], "manifest_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0/result.json", "workdir": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/runs/6addf21b-f3f3-43bf-9c8a-d7b0e11a2974/reflection_0"}, "last_good_iteration_index": 0, "rollback_info": {}, "unused_tool_picks": [], "unused_tool_files": {}, "unused_tool_outputs": {}, "unused_tool_names": [], "eval_enhanced_input": {"user_request": "Is insider buying a leading signal? Rank stocks by net insider buys in the last 90 days and run a 3-month forward performance test.", "manifest_summary": {}, "execution_result": {"success": true, "stdout": "", "stderr": ""}, "warnings": ["Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/03760e05a5101fa6d8a65f0e34028d8e388eb8cb2b367e1ea8f7a4ee27b2f992.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "Semantic key insider_trades file changed from /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/16f6f5d496a23acdd513784d5177fe484e3ef2a1e238b4bf26089a3325d2ced2.parquet to /Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet"], "datasets": [{"key": "insider_trades", "file_path": "/Users/aaryangoyal/Desktop/coffee_code/data_analytics_agent_copy_3/.agent_data_z/semantic_cache/insider_trades/95379b01c82bc38516d3d7acfc6bb2ed94f39a43bbed5c54f23d57f430ee4377.parquet", "rows": 500, "columns": 16, "dtypes": {"symbol": "object", "filingDate": "object", "transactionDate": "object", "reportingCik": "object", "companyCik": "object", "transactionType": "object", "securitiesOwned": "float64", "reportingName": "object", "typeOfOwner": "object", "acquisitionOrDisposition": "object", "directOrIndirect": "object", "formType": "object", "securitiesTransacted": "float64", "price": "float64", "securityName": "object", "url": "object"}, "nulls": {"symbol": 0, "filingDate": 0, "transactionDate": 0, "reportingCik": 0, "companyCik": 0, "transactionType": 0, "securitiesOwned": 0, "reportingName": 0, "typeOfOwner": 0, "acquisitionOrDisposition": 0, "directOrIndirect": 45, "formType": 0, "securitiesTransacted": 0, "price": 0, "securityName": 0, "url": 0}, "date_start": null, "date_end": null, "sample_head": [{"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634407663.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 27107.0, "price": 237.5579, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634381662.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 26001.0, "price": 238.322, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634377397.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 4265.0, "price": 239.4642, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634373912.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 3485.0, "price": 240.5735, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}, {"symbol": "TMUS", "filingDate": "2025-09-26", "transactionDate": "2025-09-24", "reportingCik": "0000946770", "companyCik": "0001283699", "transactionType": "S-Sale", "securitiesOwned": 634371050.0, "reportingName": "DEUTSCHE TELEKOM AG", "typeOfOwner": "director, 10 percent owner: ", "acquisitionOrDisposition": "D", "directOrIndirect": "D", "formType": "4", "securitiesTransacted": 2862.0, "price": 241.4435, "securityName": "Common Stock", "url": "https://www.sec.gov/Archives/edgar/data/1283699/000114036125036297/0001140361-25-036297-index.htm"}]}], "assumptions": [], "unused_tools_context": "\nAVAILABLE UNUSED TOOLS:\n\u2022 extract_analyst_estimates_from_fmp: Extract analyst estimates from Financial Modeling Prep\n\u2022 extract_daily_stock_data: Extract daily stock price data for a given ticker symbol, only works for a single ticker like 'AAPL' or 'MSFT' it wont work with the list of tickers like ['AAPL', 'MSFT', 'GOOGL']\n\u2022 extract_economic_data_from_fred: Extract economic data from FRED database\n\u2022 extract_esg_ratings: Make informed investment decisions based on environmental, social, and governance (ESG) performance data"}}}